{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def input_placeholder(image_size,image_channel,label_cnt):\n",
    "    with tf.name_scope(\"inputlayer\"):\n",
    "        inputs = tf.placeholder(\"float\",[None,image_size,image_size,image_channel],name='inputs')\n",
    "        labels = tf.placeholder(\"float\",[None,label_cnt],name='labels')\n",
    "        tf.summary.image(\"input_image\",inputs,max_outputs=4)\n",
    "    dropout_keep_prob = tf.placeholder(\"float\",None,name='keep_prob')\n",
    "    learning_rate = tf.placeholder(\"float\",None,name='learning_rate')\n",
    "    return inputs,labels,dropout_keep_prob,learning_rate\n",
    "def conv(inputs, kernel_size, output_num, stride_size=1, init_bias=0.0, conv_padding='SAME', stddev=0.01,\n",
    "         activation_func=tf.nn.relu):\n",
    "    input_size = inputs.get_shape().as_list()[-1]\n",
    "    conv_weights = tf.Variable(\n",
    "        tf.random_normal([kernel_size, kernel_size, input_size, output_num], dtype=tf.float32, stddev=stddev),\n",
    "        name='conv_weights')\n",
    "    conv_biases = tf.Variable(tf.constant(init_bias, shape=[output_num], dtype=tf.float32), name='conv_biases')\n",
    "    conv_layer = tf.nn.conv2d(inputs, conv_weights, [1, stride_size, stride_size, 1], padding=conv_padding)\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, conv_biases)\n",
    "    if activation_func:\n",
    "        conv_layer = activation_func(conv_layer)\n",
    "    return conv_layer\n",
    "def fc(inputs, output_size, init_bias=0.0, activation_func=tf.nn.relu, stddev=0.01):\n",
    "    input_shape = inputs.get_shape().as_list()\n",
    "    if len(input_shape) == 4:\n",
    "        fc_weights = tf.Variable(\n",
    "            tf.random_normal([input_shape[1] * input_shape[2] * input_shape[3], output_size], dtype=tf.float32,\n",
    "                             stddev=stddev),\n",
    "            name='fc_weights')\n",
    "        inputs = tf.reshape(inputs, [-1, fc_weights.get_shape().as_list()[0]])\n",
    "    else:\n",
    "        fc_weights = tf.Variable(tf.random_normal([input_shape[-1], output_size], dtype=tf.float32, stddev=stddev),\n",
    "                                 name='fc_weights')\n",
    "\n",
    "    fc_biases = tf.Variable(tf.constant(init_bias, shape=[output_size], dtype=tf.float32), name='fc_biases')\n",
    "    fc_layer = tf.matmul(inputs, fc_weights)\n",
    "    fc_layer = tf.nn.bias_add(fc_layer, fc_biases)\n",
    "    if activation_func:\n",
    "        fc_layer = activation_func(fc_layer)\n",
    "    return fc_layer\n",
    "def lrn(inputs, depth_radius=2, alpha=0.0001, beta=0.75, bias=1.0):\n",
    "    return tf.nn.local_response_normalization(inputs, depth_radius=depth_radius, alpha=alpha, beta=beta, bias=bias)\n",
    "def inference(inputs,dropout_keep_prob,label_cnt):\n",
    "    ##todo : change lrn parameters\n",
    "    #conv layer 1\n",
    "    with tf.name_scope('conv1layer'):\n",
    "        conv1 = conv(inputs,7,96,3)\n",
    "        conv1 = lrn(conv1)\n",
    "        conv1 = tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,1,1,1],padding='VALID')\n",
    "        \n",
    "    #conv layer 2\n",
    "    with tf.name_scope('conv2layer'):\n",
    "        conv2 = conv(conv1,5,256,1,1.0)\n",
    "        conv2 = lrn(conv2)\n",
    "        conv2 = tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,1,1,1],padding='VALID')\n",
    "    \n",
    "    #conv layer 3\n",
    "    with tf.name_scope('conv3layer'):\n",
    "        conv3 = conv(conv2,2,384,1)\n",
    "        \n",
    "    #conv layer 4\n",
    "    with tf.name_scope('conv4layer'):\n",
    "        conv4 = conv(conv3,3,384,1,1.0)\n",
    "        \n",
    "    #conv layer 5\n",
    "    with tf.name_scope('conv5layer'):\n",
    "        conv5 = conv(conv4,3,256,1,1.0)\n",
    "        conv5 = tf.nn.max_pool(conv5,ksize=[1,3,3,1],strides=[1,2,2,1],padding='VALID')\n",
    "        \n",
    "    #fc layer 1\n",
    "    with tf.name_scope('fc1layer'):\n",
    "        fc1 = fc(conv5,4096,1.0)\n",
    "        fc1 = tf.nn.dropout(fc1,dropout_keep_prob)\n",
    "    \n",
    "    #fc layer 2\n",
    "    with tf.name_scope('fc2layer'):\n",
    "        fc2 = fc(fc1,4096,1.0)\n",
    "        fc2 = tf.nn.dropout(fc2,dropout_keep_prob)\n",
    "    \n",
    "    #fc layer 3 - output\n",
    "    with tf.name_scope('fc3layer'):\n",
    "        return fc(fc2,label_cnt,1.0,None)\n",
    "       \n",
    "def calc_accuracy(logits,labels):\n",
    "    #accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1),tf.argmax(labels,1)),tf.float32))\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "    return accuracy\n",
    "def calc_loss(logits,labels):\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n",
    "        tf.summary.scalar('loss',loss)\n",
    "    return loss\n",
    "def train_rms_prop(loss,learning_rate,decay=0.9,momentum=0.0,epsilon=1e-10,use_locking=False,name='RMSProp'):\n",
    "    return tf.train.RMSPropOptimizer(learning_rate,decay,momentum,epsilon,use_locking,name).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "from six.moves import urllib\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "TRAIN_DATA_URL = 'http://file.hovits.com/dl/train.csv'\n",
    "TEST_DATA_URL = 'http://file.hovits.com/dl/test.csv'\n",
    "\n",
    "def dense_to_one_hot(labels_dense,num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels,num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "def load_mnist_train(validation_size=2000,batch_size=128):\n",
    "    download_train()\n",
    "    data = pd.read_csv(FLAGS.train_path)\n",
    "    images = data.iloc[:,1:].values\n",
    "    images = images.astype(np.float)\n",
    "    images = np.multiply(images,1.0/255.0)\n",
    "    image_size = images.shape[1]\n",
    "    image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "    images = images.reshape(-1,image_width,image_height,1)\n",
    "    labels_flat = data.iloc[:,0].values.ravel()\n",
    "    labels_count = np.unique(labels_flat).shape[0]\n",
    "    labels = dense_to_one_hot(labels_flat,labels_count)\n",
    "    labels = labels.astype(np.uint8)\n",
    "    validation_images = images[:validation_size]\n",
    "    validation_labels = labels[:validation_size]\n",
    "    train_images = images[validation_size:]\n",
    "    train_labels = labels[validation_size:]\n",
    "    train_range = list(zip(range(0,len(train_images),batch_size),range(batch_size,len(train_images),batch_size)))\n",
    "    if len(train_images) % batch_size > 0:\n",
    "        train_range.append((train_range[-1][1],len(train_images)))\n",
    "        \n",
    "    validation_indices = np.arange(len(validation_images))\n",
    "    return train_images,train_labels,train_range,validation_images,validation_labels,validation_indices\n",
    "def load_mnist_test(batch_size=128):\n",
    "    download_test()\n",
    "    data = pd.read_csv(FLAGS.test_path)\n",
    "    images = data.iloc[:,1:].values\n",
    "    images = images.astype(np.float)\n",
    "    images = np.multiply(images,1.0/255.0)\n",
    "    image_size = images.shape[1]\n",
    "    image_width = image_height = np.ceil(np.sqrt(image_size)).astype(np.uint8)\n",
    "    images = images.reshape(-1,image_width,image_height,1)\n",
    "    image_account = len(images)\n",
    "    labels_flat = data.iloc[:,0].values.ravel()\n",
    "    labels_count = np.unique(labels_flat).shape[0]\n",
    "    labels = dense_to_one_hot(labels_flat,labels_count)\n",
    "    labels = labels.astype(np.uint8)\n",
    "    test_range = list(zip(range(0,image_account,batch_size),range(batch_size,image_account,batch_size)))\n",
    "    if image_account % batch_size > 0:\n",
    "        test_range.append((test_range[-1][1],image_account))  \n",
    "    return images,labels,test_range,image_account\n",
    "def shuffle_validation(validation_indices,batch_size):\n",
    "    np.random.shuffle(validation_indices)\n",
    "    return validation_indices[0:batch_size]\n",
    "def download_train():\n",
    "    statinfo = download(FLAGS.train_path,TRAIN_DATA_URL)\n",
    "    if statinfo:\n",
    "        print('Training data is successfully downloaded',statinfo.st_size,'bytes.')\n",
    "    else:\n",
    "        print('Training data was already downloaded')\n",
    "def download_test():\n",
    "    statinfo = download(FLAGS.test_path,TEST_DATA_URL)\n",
    "    if statinfo:\n",
    "        print('Test data is successuflly downloaded',statinfo.st_size,'bytes.')\n",
    "    else:\n",
    "        print('Test data was already downloaded')\n",
    "def download(path,url):\n",
    "    if not os.path.exists(path):\n",
    "        if not os.path.isdir(os.path.basename(path)):\n",
    "            os.makedirs(os.path.basename(path))\n",
    "            \n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write(\n",
    "                '\\r>> Downloading %s %.1f%%' % (path, float(count * block_size) / float(total_size) * 100.0))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        file_path, _ = urllib.request.urlretrieve(url, path, _progress)\n",
    "        print()\n",
    "        return os.stat(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train or Test AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('training_epoch',30,\"training epoch\")\n",
    "tf.app.flags.DEFINE_integer('batch_size',128,\"batch_size\")\n",
    "tf.app.flags.DEFINE_integer('validation_interval',100,\"validation interval\")\n",
    "tf.app.flags.DEFINE_float('dropout_keep_prob',0.5,\"dropout_keep_prob\")\n",
    "tf.app.flags.DEFINE_float('learning_rate',0.001,'learning rate')\n",
    "tf.app.flags.DEFINE_float('rms_decay',0.9,\"rms optimizer decay\")\n",
    "tf.app.flags.DEFINE_float('weight_decay',0.0005,\"L2 regularization weight decay\")\n",
    "tf.app.flags.DEFINE_string('train_path','./data/train.csv',\"path to download training data\")\n",
    "tf.app.flags.DEFINE_string('test_path','./data/test.csv',\"path to download test data\")\n",
    "tf.app.flags.DEFINE_integer('validation_size',2000,\"validation size in training data\")\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir','./checkpoint/',\"path to checkpoint file\")\n",
    "tf.app.flags.DEFINE_boolean('is_train',False,\"True for train,False for test\")\n",
    "tf.app.flags.DEFINE_string('test_result','./output/test_result.csv',\"testing output file path\")\n",
    "\n",
    "image_size = 28\n",
    "image_channel = 1\n",
    "label_cnt = 10\n",
    "\n",
    "def train():\n",
    "    #build graph\n",
    "    inputs,labels,dropout_keep_prob,learning_rate = input_placeholder(image_size,image_channel,label_cnt)\n",
    "    logits = inference(inputs,dropout_keep_prob,label_cnt)\n",
    "    accuracy = calc_accuracy(logits,labels)\n",
    "    loss = calc_loss(logits,labels)\n",
    "    train = tf.train.RMSPropOptimizer(learning_rate,FLAGS.rms_decay).minimize(loss)\n",
    "    #session\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    #ready for summary\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('./summary/train',sess.graph)\n",
    "    validation_writer = tf.summary.FileWriter('./summary/validation')\n",
    "    #tf saver\n",
    "    saver = tf.train.Saver()\n",
    "    if os.path.isfile(FLAGS.checkpoint_name):\n",
    "        saver.restore(sess,FLAGS.checkpoint_name)\n",
    "    total_start_time = time.time()\n",
    "    #load mnist data\n",
    "    train_images,train_labels,train_range,validation_images,validation_labels,validation_indices = load_mnist_train(\n",
    "        FLAGS.validation_size,FLAGS.batch_size)\n",
    "    total_train_len = len(train_images)\n",
    "    i = 0\n",
    "    cur_learning_rate = FLAGS.learning_rate\n",
    "    for epoch in range(FLAGS.training_epoch):\n",
    "        if epoch % 10 == 0 and epoch > 0:\n",
    "            cur_learning_rate /= 10\n",
    "        epoch_start_time = time.time()\n",
    "        for start,end in train_range:\n",
    "            batch_start_time = time.time()\n",
    "            train_x = train_images[start:end]\n",
    "            train_y = train_labels[start:end]\n",
    "            if i % 20 == 0:\n",
    "                summary,_,loss_result = sess.run([merged,train,loss],feed_dict={inputs:train_x,labels:train_y,\n",
    "                                                                                dropout_keep_prob:FLAGS.dropout_keep_prob,\n",
    "                                                                                learning_rate:cur_learning_rate})\n",
    "                train_writer.add_summary(summary,i)\n",
    "                print('[%s][training][epoch %d,step %d exec %.2f seconds] [file:%5d - %5d / %5d] loss: %3.10f'%(\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\"),epoch, i,(time.time()-batch_start_time),start,end,total_train_len,loss_result))\n",
    "            else:\n",
    "                _,loss_result = sess.run([train,loss],feed_dict={inputs:train_x,labels:train_y,\n",
    "                                                                dropout_keep_prob:FLAGS.dropout_keep_prob,\n",
    "                                                                learning_rate:cur_learning_rate})\n",
    "            \n",
    "            if i % FLAGS.validation_interval == 0 and i > 0:\n",
    "                validation_start_time = time.time()\n",
    "                shuffle_indices = shuffle_validation(validation_indices,FLAGS.batch_size)\n",
    "                validation_x = validation_images[shuffle_indices]\n",
    "                validation_y = validation_labels[shuffle_indices]\n",
    "                summary,accuracy_result,loss_result = sess.run([merged,accuracy,loss],feed_dict={inputs:validation_x,labels:validation_y,\n",
    "                                                                                                dropout_keep_prob:1.0})\n",
    "                validation_writer.add_summary(summary,i)\n",
    "                print('[%s][validation][epoch %d, step %d exec %.2f seconds] accuracy : %1.3f,loss:%3.10f'%(\n",
    "                    time.strftime(\"%Y-%m-%d %H:%M:%S\"),epoch,i,(time.time()-validation_start_time),accuracy_result,loss_result))\n",
    "            \n",
    "            i += 1\n",
    "        print(\"[%s][epoch exec %s seconds] epoch : %d\"%(time.strftime(\"%Y-%m-%d %H:%M:%S\"),(time.time()-epoch_start_time),epoch))\n",
    "        saver.save(sess,FLAGS.checkpoint_name),\n",
    "    print(\"[%s][total exec %s seconds]\"%(time.strftime(\"%Y-%m-%d %H:%M:%S\"),(time.time()-total_start_time)))\n",
    "    train_writer.close()\n",
    "    validation_writer.close()\n",
    "    \n",
    "def test():\n",
    "   #build graph\n",
    "    inputs,labels,dropout_keep_prob,learning_rate = input_placeholder(image_size,image_channel,label_cnt)\n",
    "    logits = inference(inputs,dropout_keep_prob,label_cnt)\n",
    "    predict = tf.argmax(logits,1)\n",
    "    #session\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    #tf saver\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored...\")\n",
    "    #load test data\n",
    "    test_images,test_labels,test_range,test_account = load_mnist_test(FLAGS.batch_size)\n",
    "    #ready for result file\n",
    "    test_result_file = open(FLAGS.test_result,'w')\n",
    "    csv_writer = csv.writer(test_result_file)\n",
    "    csv_writer.writerow(['ImageId','Predict','Label'])\n",
    "    total_start_time = time.time()\n",
    "    test_total_account = 0\n",
    "    step = 0\n",
    "    for file_start,file_end in test_range:\n",
    "        test_range_account = 0\n",
    "        test_x = test_images[file_start:file_end]\n",
    "        test_y = test_labels[file_start:file_end]\n",
    "        predict_label = sess.run(predict,feed_dict={inputs:test_x,dropout_keep_prob:1.0})\n",
    "        for i,cur_predict in enumerate(predict_label):\n",
    "            real_label = np.argmax(test_y[i])\n",
    "            csv_writer.writerow([step,cur_predict,real_label])\n",
    "            if cur_predict == real_label:\n",
    "                test_range_account += 1\n",
    "            step += 1\n",
    "            if step % 256 == 0:\n",
    "                print('[Test file: %4d-%4d,the %d file is predicted %d,the label is %d]'%(file_start,file_end,i,cur_predict,real_label))\n",
    "                print('Test file: %5d - %5d,Accuracy is:%f'%(file_start,file_end,test_range_account / float(file_end - file_start)))\n",
    "        test_total_account += test_range_account\n",
    "    print(\"[%s][total exec %s second,Accuracy is:%f]\"%(time.strftime(\"%Y-%m-%d %H:%M:%S\"),(time.time()-total_start_time),\n",
    "                                                       test_total_account / float(test_account)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoint/var.ckpt\n",
      "Model restored...\n",
      "Test data was already downloaded\n",
      "[Test file:  128- 256,the 127 file is predicted 2,the label is 2]\n",
      "Test file:   128 -   256,Accuracy is:0.992188\n",
      "[Test file:  384- 512,the 127 file is predicted 2,the label is 2]\n",
      "Test file:   384 -   512,Accuracy is:1.000000\n",
      "[Test file:  640- 768,the 127 file is predicted 1,the label is 1]\n",
      "Test file:   640 -   768,Accuracy is:0.992188\n",
      "[Test file:  896-1024,the 127 file is predicted 4,the label is 4]\n",
      "Test file:   896 -  1024,Accuracy is:0.992188\n",
      "[Test file: 1152-1280,the 127 file is predicted 1,the label is 1]\n",
      "Test file:  1152 -  1280,Accuracy is:0.992188\n",
      "[Test file: 1408-1536,the 127 file is predicted 6,the label is 6]\n",
      "Test file:  1408 -  1536,Accuracy is:1.000000\n",
      "[Test file: 1664-1792,the 127 file is predicted 7,the label is 7]\n",
      "Test file:  1664 -  1792,Accuracy is:1.000000\n",
      "[Test file: 1920-2048,the 127 file is predicted 7,the label is 7]\n",
      "Test file:  1920 -  2048,Accuracy is:1.000000\n",
      "[Test file: 2176-2304,the 127 file is predicted 0,the label is 0]\n",
      "Test file:  2176 -  2304,Accuracy is:0.992188\n",
      "[Test file: 2432-2560,the 127 file is predicted 3,the label is 3]\n",
      "Test file:  2432 -  2560,Accuracy is:0.992188\n",
      "[Test file: 2688-2816,the 127 file is predicted 1,the label is 1]\n",
      "Test file:  2688 -  2816,Accuracy is:1.000000\n",
      "[Test file: 2944-3072,the 127 file is predicted 8,the label is 8]\n",
      "Test file:  2944 -  3072,Accuracy is:1.000000\n",
      "[Test file: 3200-3328,the 127 file is predicted 7,the label is 7]\n",
      "Test file:  3200 -  3328,Accuracy is:0.992188\n",
      "[Test file: 3456-3584,the 127 file is predicted 0,the label is 0]\n",
      "Test file:  3456 -  3584,Accuracy is:0.984375\n",
      "[Test file: 3712-3840,the 127 file is predicted 9,the label is 9]\n",
      "Test file:  3712 -  3840,Accuracy is:1.000000\n",
      "[Test file: 3968-4096,the 127 file is predicted 8,the label is 8]\n",
      "Test file:  3968 -  4096,Accuracy is:0.992188\n",
      "[Test file: 4224-4352,the 127 file is predicted 3,the label is 3]\n",
      "Test file:  4224 -  4352,Accuracy is:1.000000\n",
      "[Test file: 4480-4608,the 127 file is predicted 8,the label is 8]\n",
      "Test file:  4480 -  4608,Accuracy is:0.984375\n",
      "[Test file: 4736-4864,the 127 file is predicted 1,the label is 1]\n",
      "Test file:  4736 -  4864,Accuracy is:0.992188\n",
      "[Test file: 4992-5120,the 127 file is predicted 2,the label is 2]\n",
      "Test file:  4992 -  5120,Accuracy is:1.000000\n",
      "[Test file: 5248-5376,the 127 file is predicted 8,the label is 8]\n",
      "Test file:  5248 -  5376,Accuracy is:1.000000\n",
      "[Test file: 5504-5632,the 127 file is predicted 5,the label is 5]\n",
      "Test file:  5504 -  5632,Accuracy is:1.000000\n",
      "[Test file: 5760-5888,the 127 file is predicted 4,the label is 4]\n",
      "Test file:  5760 -  5888,Accuracy is:1.000000\n",
      "[Test file: 6016-6144,the 127 file is predicted 4,the label is 4]\n",
      "Test file:  6016 -  6144,Accuracy is:1.000000\n",
      "[Test file: 6272-6400,the 127 file is predicted 0,the label is 0]\n",
      "Test file:  6272 -  6400,Accuracy is:1.000000\n",
      "[Test file: 6528-6656,the 127 file is predicted 9,the label is 9]\n",
      "Test file:  6528 -  6656,Accuracy is:0.984375\n",
      "[Test file: 6784-6912,the 127 file is predicted 2,the label is 2]\n",
      "Test file:  6784 -  6912,Accuracy is:1.000000\n",
      "[Test file: 7040-7168,the 127 file is predicted 0,the label is 0]\n",
      "Test file:  7040 -  7168,Accuracy is:1.000000\n",
      "[Test file: 7296-7424,the 127 file is predicted 1,the label is 1]\n",
      "Test file:  7296 -  7424,Accuracy is:1.000000\n",
      "[Test file: 7552-7680,the 127 file is predicted 9,the label is 9]\n",
      "Test file:  7552 -  7680,Accuracy is:1.000000\n",
      "[Test file: 7808-7936,the 127 file is predicted 3,the label is 3]\n",
      "Test file:  7808 -  7936,Accuracy is:1.000000\n",
      "[Test file: 8064-8192,the 127 file is predicted 5,the label is 5]\n",
      "Test file:  8064 -  8192,Accuracy is:1.000000\n",
      "[Test file: 8320-8448,the 127 file is predicted 0,the label is 0]\n",
      "Test file:  8320 -  8448,Accuracy is:0.976562\n",
      "[Test file: 8576-8704,the 127 file is predicted 2,the label is 2]\n",
      "Test file:  8576 -  8704,Accuracy is:1.000000\n",
      "[Test file: 8832-8960,the 127 file is predicted 4,the label is 4]\n",
      "Test file:  8832 -  8960,Accuracy is:1.000000\n",
      "[Test file: 9088-9216,the 127 file is predicted 9,the label is 9]\n",
      "Test file:  9088 -  9216,Accuracy is:1.000000\n",
      "[Test file: 9344-9472,the 127 file is predicted 9,the label is 9]\n",
      "Test file:  9344 -  9472,Accuracy is:1.000000\n",
      "[Test file: 9600-9728,the 127 file is predicted 4,the label is 4]\n",
      "Test file:  9600 -  9728,Accuracy is:1.000000\n",
      "[Test file: 9856-9984,the 127 file is predicted 1,the label is 1]\n",
      "Test file:  9856 -  9984,Accuracy is:1.000000\n",
      "[2018-07-15 17:32:34][total exec 3.24857234954834 second,Accuracy is:0.996400]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if FLAGS.is_train:\n",
    "        train()\n",
    "    else:\n",
    "        test()\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_data = pd.read_csv('./data/train.csv')\n",
    "input_images = input_data.iloc[:,1:].values\n",
    "inputlabels_flat = input_data.iloc[:,0].values.ravel()\n",
    "inputlabels_count = np.unique(inputlabels_flat).shape[0]\n",
    "print(inputlabels_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
